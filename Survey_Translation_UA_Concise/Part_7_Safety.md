# Частина 7: Безпека моделей коду

З розвитком можливостей Code LLM питання безпеки генерованого коду стає критичним. Моделі можуть поширювати вразливості, вивчені з відкритих репозиторіїв, або ставати інструментами для кібератак.

```mermaid
graph TD
    A[Безпека Code LLM] --> B[Навчання (Pre-training)]
    A --> C[Узгодження (Post-training)]
    A --> D[Red-Teaming]
    A --> E[Безпека агентів]

    B --> B1[Очищення даних, Ліцензійна відповідність, Конфіденційність]
    C --> C1[SFT з безпеки, Оптимізація уподобань (DPO/LPO), RLHF]
    D --> D1[Джейлбрейки, Маніпуляція промптами, Введення команд]
    E --> E1[Пісочниці (Sandboxing), Контроль у реальному часі, Верифікація намірів]
```
*Рисунок 23: Таксономія ключових вимірів забезпечення безпеки великих мовних моделей коду.*

---

## 7.1 Безпека на етапі попереднього навчання (Pre-training)

Основна проблема полягає в тому, що моделі навчаються на мільйонах публічних репозиторіїв, які містять небезпечні паттерни коду.
-   **Походження даних та ліцензії:** Використання проектів як **The Stack** (BigCode) для фільтрації коду за ліцензіями (MIT, Apache тощо).
-   **Очищення даних:** Видалення секретів (API ключі, паролі) та коду з відомими вразливостями (CWE).
-   **Боротьба з упередженнями (Bias):** Моделі можуть наслідувати соціальні стереотипи через імена змінних або коментарі. Використовуються бенчмарки на кшталт **FairCoder**.

## 7.2 Узгодження після навчання (Post-training)

Оскільки моделі за замовчуванням схильні генерувати "найбільш ймовірний" (часто вразливий) код, потрібне додаткове навчання.
-   **SFT (Supervised Fine-Tuning):** Навчання на парах "вразливий код -> виправлений код".
-   **Localized Preference Optimization (LPO):** На відміну від стандартного DPO, LPO фокусується лише на критичних токенах, що викликають вразливість, зберігаючи загальну якість коду.
-   **RLHF та RLAIF:** Використання фідбеку від людей або інших моделей-"вчителів" для формування винагороди за безпечний код.

## 7.3 Red-teaming (Атаки на моделі)

-   **Маніпуляція промптами:** Спеціальні "джейлбрейки" (наприклад, рольові ігри "DAN"), що змушують модель ігнорувати фільтри.
-   **Непряма ін’єкція промптів (Indirect Prompt Injection):** Коли агент зчитує отруєні дані з зовнішнього джерела (наприклад, коментар у веб-сторінці), які містять приховані команди.
-   **Автоматизована експлуатація (AVDE):** Перетворення агента на автономний інструмент для пентестингу.

## 7.4 Безпека агентних систем

Коли ШІ отримує доступ до терміналу та інструментів, ризики зростають.
-   **Ізольовані середовища:** Використання Docker, MicroVM (Firecracker) або gVisor для обмеження доступу до хост-системи.
-   **Runtime Oversight:** Моніторинг дій агента в реальному часі ("Guardian Agent"), який може заблокувати небезпечну команду перед виконанням.
-   **Rollback (Ctrl-Z):** Можливість відкотити дії агента, якщо вони визнані шкідливими.
